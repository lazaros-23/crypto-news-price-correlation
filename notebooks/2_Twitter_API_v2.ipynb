{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "computational-phone",
   "metadata": {},
   "source": [
    "# Twitter API v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-heavy",
   "metadata": {},
   "source": [
    "* **Tutorial: How to analyze the sentiment of your own Tweets**   \n",
    "https://developer.twitter.com/en/docs/tutorials/how-to-analyze-the-sentiment-of-your-own-tweets\n",
    "\n",
    "\n",
    "* **Tutorial: Search tweets**    \n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction\n",
    "\n",
    "\n",
    "* **Tutorial: Comparing Features**   \n",
    "https://developer.twitter.com/en/docs/twitter-api/search-overview\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/search/migrate \n",
    "\n",
    "\n",
    "\n",
    "* **Twitter API v2**   \n",
    "https://github.com/twitterdev/Twitter-API-v2-sample-code  \n",
    "https://github.com/twitterdev/search-tweets-python/tree/v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-stack",
   "metadata": {},
   "source": [
    "**Search tweets, using Twitter API**\n",
    "\n",
    "1. **Recent search** - last 7 days (Standard and Academic Research product tracks)\n",
    "\n",
    "> This endpoint can deliver up to 100 Tweets per request in reverse-chronological order, and pagination tokens are provided for paging through large sets of matching Tweets. \n",
    "\n",
    "\n",
    "2. **Full-archive search** (Academic Research product track only)\n",
    "\n",
    "> At this time, the v2 full-archive search endpoint is only available via the Academic Research product track. The endpoint allows you to programmatically access public Tweets from the complete archive dating back to the first Tweet in March 2006, based on your search query. This endpoint can deliver up to 500 Tweets per request in reverse-chronological order, and pagination tokens are provided for paging through large sets of matching Tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-consequence",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-arrow",
   "metadata": {},
   "source": [
    "**Tutorial:** https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a\n",
    "\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Introduction\n",
    "2. Prerequisites to Start\n",
    "3. Bearer Token\n",
    "4. Create Headers\n",
    "5. Create URL\n",
    "\t- search url\n",
    "\t- query params\n",
    "6. Connect to Endpoint\n",
    "7. Call the API endpoint\n",
    "\t- data\n",
    "\t- meta\n",
    "8. Save Results to CSV\n",
    "9. Looping Through Requests\n",
    "\n",
    "> If we just send a request to collect tweets between the 1st of January 2020 and the 31st of December 2020, we will hit our cap very quickly without having a good distribution from all 12 months.\n",
    "So what we can do is, we can set a limit for tweets we want to collect per month, so that if we reach the specific cap at one month, we move on to the next one.\n",
    "\n",
    "* A For-loop that goes over the months/weeks/days we want to cover (Depending on how it is set)\n",
    "\n",
    "* A While-loop that controls the maximum number of tweets we want to collect per time period.\n",
    "\n",
    "* Notice that a time.sleep() is added between calls to ensure you are not just spamming the API with requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extensive-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prerequisites to Start\n",
    "\n",
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "helpful-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Bearer token\n",
    "def auth():\n",
    "    with open(\"../config.yaml\") as file:\n",
    "        passwords = yaml.safe_load(file)\n",
    "    return passwords[\"search_tweets_api\"][\"bearer_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "searching-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create Headers\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "direct-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create url\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'tweet.fields': 'id,text,author_id,created_at,public_metrics',\n",
    "                    # 'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    # 'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    # 'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    # 'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informal-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Connect to endpoint\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "medium-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "## // Inputs for the request //\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "\n",
    "# keyword = \"ethereum -is:retweet lang:en\"\n",
    "# keyword = \"(ethereum OR ether OR eth) lang:en -is:retweet -is:reply\"\n",
    "                            keyword = \"(audiusproject OR audius OR $audio OR audiocoin) lang:en -is:retweet -is:reply\"\n",
    "\n",
    "start_time = \"2021-03-01T00:00:00.000Z\"\n",
    "end_time = \"2021-03-31T00:00:00.000Z\"\n",
    "max_results = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "twelve-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Response Code: 200\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"author_id\": \"19953652\",\n",
      "            \"created_at\": \"2021-03-30T23:57:36.000Z\",\n",
      "            \"id\": \"1377047388590899203\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 1,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"I just hit over 100 followers on #Audius! https://t.co/WLjNXmRCLz #NewMusic\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1117417373940711426\",\n",
      "            \"created_at\": \"2021-03-30T23:57:11.000Z\",\n",
      "            \"id\": \"1377047282785353737\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"https://t.co/1uPc7jThCw hmmm what  is this\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"904521983777492993\",\n",
      "            \"created_at\": \"2021-03-30T23:55:26.000Z\",\n",
      "            \"id\": \"1377046843243253765\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 1,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"My Track WHERE ARE YOU NOW [It's Goin' Down]  has more than 10 Favorites on @AudiusProject #Audius \\nCheck it out! https://t.co/fK2DGxU4Z3\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1141208687001780225\",\n",
      "            \"created_at\": \"2021-03-30T23:55:11.000Z\",\n",
      "            \"id\": \"1377046780622348288\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"$AUDIO\\n\\u2705  Increased 3.56% in 1.0 hour(s)\\n\\ud83d\\udcb5 Price - 3.45734000 USDT\\n\\u23f1\\ufe0f [30 Feb] - 23:55:11 UTC\\n#AUDIO #AUDIOUSDT #CryptoBOT\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1359553186906984450\",\n",
      "            \"created_at\": \"2021-03-30T23:51:48.000Z\",\n",
      "            \"id\": \"1377045930751459328\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"Started a new day, a new grind make e choke  \\ud83d\\ude4f\\ud83d\\ude4f\\ud83d\\ude4f\\n@fireboydml \\n@davido @TwitterMusic \\n@AudiusProject\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1367496296328798214\",\n",
      "            \"created_at\": \"2021-03-30T23:50:59.000Z\",\n",
      "            \"id\": \"1377045725918466052\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 2,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"I just hit over 25 followers on @AudiusProject #Audius! https://t.co/GSWBbBDfUm\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"299321216\",\n",
      "            \"created_at\": \"2021-03-30T23:50:18.000Z\",\n",
      "            \"id\": \"1377045553255682053\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"I just joined @AudiusProject and uploaded my first track! Check out my profile here:  https://t.co/hH0CiV6lgl\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"42659681\",\n",
      "            \"created_at\": \"2021-03-30T23:49:08.000Z\",\n",
      "            \"id\": \"1377045257037230080\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 8,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 1,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"I got this tune up on @AudiusProject  -  Xinobi feat. Ivy - The Moment #Audius https://t.co/BM3RM2RwbU \\n\\ncome visit!\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"305823361\",\n",
      "            \"created_at\": \"2021-03-30T23:48:06.000Z\",\n",
      "            \"id\": \"1377044996738678784\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 0,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 0,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"Check out my new album, A.B.C AKA Always Be Closing on @AudiusProject #Audius https://t.co/fMQZbczt40 \\n#audius #instagram #twitter #lilbaby #drake #migos #music\"\n",
      "        },\n",
      "        {\n",
      "            \"author_id\": \"1253659248703279104\",\n",
      "            \"created_at\": \"2021-03-30T23:45:27.000Z\",\n",
      "            \"id\": \"1377044329584283653\",\n",
      "            \"public_metrics\": {\n",
      "                \"like_count\": 2,\n",
      "                \"quote_count\": 0,\n",
      "                \"reply_count\": 1,\n",
      "                \"retweet_count\": 0\n",
      "            },\n",
      "            \"text\": \"Been curating playlists on @AudiusProject for a minute... Now that there's rewards guess it's time to tell you about them! [Thread]\"\n",
      "        }\n",
      "    ],\n",
      "    \"meta\": {\n",
      "        \"newest_id\": \"1377047388590899203\",\n",
      "        \"next_token\": \"b26v89c19zqg8o3fosqtycrbq6s1wae6p9dk79rfp9qil\",\n",
      "        \"oldest_id\": \"1377044329584283653\",\n",
      "        \"result_count\": 10\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## // Call the API //\n",
    "url = create_url(keyword, start_time,end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])\n",
    "print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vulnerable-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Results\n",
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        # 'author_id', 'created_at', 'tweet_id', 'text', 'like_count', 'quote_count', 'reply_count', 'retweet_count'\n",
    "        res = [author_id, created_at, tweet_id, text, like_count, quote_count, reply_count, retweet_count]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "geological-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Tweets added from this response:  10\n"
     ]
    }
   ],
   "source": [
    "# Create file\n",
    "csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow([ 'author_id', 'created_at', 'tweet_id', 'text', 'like_count', 'quote_count', 'reply_count', 'retweet_count'])\n",
    "csvFile.close()\n",
    "\n",
    "append_to_csv(json_response=json_response, fileName=\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "played-capability",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## // Store collected tweets in a pickle file //\n",
    "\n",
    "# import pandas as pd\n",
    "# from pandas import json_normalize \n",
    "# df = json_normalize(json_response, 'data')\n",
    "# display(df)\n",
    "# print(df.columns)\n",
    "# df.to_pickle('data/collected_tweets.pkl')\n",
    "# df = pd.read_pickle('data/collected_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "searching-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "## // Store the data in Human readable format (collected_tweets.txt) // \n",
    "# but also store the data in a pickle dataframe ready to be processed using pandas.\n",
    "\n",
    "# keys = json_response[\"data\"][0].keys()\n",
    "# print(keys)\n",
    "\n",
    "# with open('../data/collected_tweets.txt', 'a') as outfile:\n",
    "#     for i in range(len(json_response[\"data\"])):\n",
    "#         json.dump(json_response[\"data\"][i], outfile)\n",
    "#         outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-picking",
   "metadata": {},
   "source": [
    "# Pagination (Loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-jones",
   "metadata": {},
   "source": [
    "**Query**\n",
    "\n",
    "* Να περιέχει αυτά τα keywords: (ethereum OR ether OR eth)\n",
    "* Να είναι γραμμένο στα αγγλικά (lang:en)\n",
    "* Να μην είναι retweet (-is:retweet)\n",
    "* Να μην ειναι answer (-is:reply)\n",
    "* Να μην είναι διαφημιστικό (-is:nullcast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-disclosure",
   "metadata": {},
   "source": [
    "**How many tweets?**\n",
    "\n",
    "* 24 hours, 365 days = 8760 hours\n",
    "* 100-200 tweets per hour\n",
    "\n",
    "* 200 * 24 * 365 = 1_752_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pediatric-shock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timestamps: 2952\n"
     ]
    }
   ],
   "source": [
    "date_list = []\n",
    "\n",
    "for month in range(5, 9):\n",
    "    if month in [2,4,6,9,11]:\n",
    "        for day in range(1, 31):\n",
    "            for hour in range(0,24):\n",
    "                month = str(month).zfill(2)\n",
    "                day = str(day).zfill(2)\n",
    "                hour = str(hour).zfill(2)\n",
    "                date = f\"2021-{month}-{day}T{hour}:00:00.000Z\"\n",
    "\n",
    "                # print(date)\n",
    "                date_list.append(date)\n",
    "    elif month in [1,3,5,7,8,10,12]:\n",
    "        for day in range(1, 32):\n",
    "            for hour in range(0,24):\n",
    "                month = str(month).zfill(2)\n",
    "                day = str(day).zfill(2)\n",
    "                hour = str(hour).zfill(2)\n",
    "                date = f\"2021-{month}-{day}T{hour}:00:00.000Z\"\n",
    "\n",
    "                # print(date)\n",
    "                date_list.append(date)\n",
    "\n",
    "print(f\"Total timestamps: {len(date_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceramic-craps",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-01T00:00:00.000Z</td>\n",
       "      <td>2021-05-01T01:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-01T01:00:00.000Z</td>\n",
       "      <td>2021-05-01T02:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-01T02:00:00.000Z</td>\n",
       "      <td>2021-05-01T03:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-01T03:00:00.000Z</td>\n",
       "      <td>2021-05-01T04:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-01T04:00:00.000Z</td>\n",
       "      <td>2021-05-01T05:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-05-01T05:00:00.000Z</td>\n",
       "      <td>2021-05-01T06:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-05-01T06:00:00.000Z</td>\n",
       "      <td>2021-05-01T07:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-05-01T07:00:00.000Z</td>\n",
       "      <td>2021-05-01T08:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-05-01T08:00:00.000Z</td>\n",
       "      <td>2021-05-01T09:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-05-01T09:00:00.000Z</td>\n",
       "      <td>2021-05-01T10:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-05-01T10:00:00.000Z</td>\n",
       "      <td>2021-05-01T11:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-05-01T11:00:00.000Z</td>\n",
       "      <td>2021-05-01T12:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-05-01T12:00:00.000Z</td>\n",
       "      <td>2021-05-01T13:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-05-01T13:00:00.000Z</td>\n",
       "      <td>2021-05-01T14:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-05-01T14:00:00.000Z</td>\n",
       "      <td>2021-05-01T15:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-05-01T15:00:00.000Z</td>\n",
       "      <td>2021-05-01T16:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-05-01T16:00:00.000Z</td>\n",
       "      <td>2021-05-01T17:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-05-01T17:00:00.000Z</td>\n",
       "      <td>2021-05-01T18:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-05-01T18:00:00.000Z</td>\n",
       "      <td>2021-05-01T19:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-05-01T19:00:00.000Z</td>\n",
       "      <td>2021-05-01T20:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-05-01T20:00:00.000Z</td>\n",
       "      <td>2021-05-01T21:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-05-01T21:00:00.000Z</td>\n",
       "      <td>2021-05-01T22:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-05-01T22:00:00.000Z</td>\n",
       "      <td>2021-05-01T23:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-05-01T23:00:00.000Z</td>\n",
       "      <td>2021-05-02T00:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-05-02T00:00:00.000Z</td>\n",
       "      <td>2021-05-02T01:00:00.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       start                       end\n",
       "0   2021-05-01T00:00:00.000Z  2021-05-01T01:00:00.000Z\n",
       "1   2021-05-01T01:00:00.000Z  2021-05-01T02:00:00.000Z\n",
       "2   2021-05-01T02:00:00.000Z  2021-05-01T03:00:00.000Z\n",
       "3   2021-05-01T03:00:00.000Z  2021-05-01T04:00:00.000Z\n",
       "4   2021-05-01T04:00:00.000Z  2021-05-01T05:00:00.000Z\n",
       "5   2021-05-01T05:00:00.000Z  2021-05-01T06:00:00.000Z\n",
       "6   2021-05-01T06:00:00.000Z  2021-05-01T07:00:00.000Z\n",
       "7   2021-05-01T07:00:00.000Z  2021-05-01T08:00:00.000Z\n",
       "8   2021-05-01T08:00:00.000Z  2021-05-01T09:00:00.000Z\n",
       "9   2021-05-01T09:00:00.000Z  2021-05-01T10:00:00.000Z\n",
       "10  2021-05-01T10:00:00.000Z  2021-05-01T11:00:00.000Z\n",
       "11  2021-05-01T11:00:00.000Z  2021-05-01T12:00:00.000Z\n",
       "12  2021-05-01T12:00:00.000Z  2021-05-01T13:00:00.000Z\n",
       "13  2021-05-01T13:00:00.000Z  2021-05-01T14:00:00.000Z\n",
       "14  2021-05-01T14:00:00.000Z  2021-05-01T15:00:00.000Z\n",
       "15  2021-05-01T15:00:00.000Z  2021-05-01T16:00:00.000Z\n",
       "16  2021-05-01T16:00:00.000Z  2021-05-01T17:00:00.000Z\n",
       "17  2021-05-01T17:00:00.000Z  2021-05-01T18:00:00.000Z\n",
       "18  2021-05-01T18:00:00.000Z  2021-05-01T19:00:00.000Z\n",
       "19  2021-05-01T19:00:00.000Z  2021-05-01T20:00:00.000Z\n",
       "20  2021-05-01T20:00:00.000Z  2021-05-01T21:00:00.000Z\n",
       "21  2021-05-01T21:00:00.000Z  2021-05-01T22:00:00.000Z\n",
       "22  2021-05-01T22:00:00.000Z  2021-05-01T23:00:00.000Z\n",
       "23  2021-05-01T23:00:00.000Z  2021-05-02T00:00:00.000Z\n",
       "24  2021-05-02T00:00:00.000Z  2021-05-02T01:00:00.000Z"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create start_list and end_list\n",
    "start_list = date_list\n",
    "end_list = date_list[1:]\n",
    "\n",
    "# Sanity check\n",
    "pd.DataFrame(data = {\"start\":start_list[:25],\n",
    "                    \"end\":end_list[:25]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accessible-astronomy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting timestamp: 2021-05-01T00:00:00.000Z\n",
      "ending timestamp: 2021-08-31T23:00:00.000Z\n"
     ]
    }
   ],
   "source": [
    "print(\"starting timestamp:\", start_list[0])\n",
    "print(\"ending timestamp:\", start_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "thermal-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_list =    ['2021-01-01T00:00:00.000Z',\n",
    "#                  '2021-02-01T00:00:00.000Z',\n",
    "#                  '2021-03-01T00:00:00.000Z',\n",
    "#                  '2021-04-01T00:00:00.000Z',\n",
    "#                  '2021-05-01T00:00:00.000Z',\n",
    "#                  '2021-06-01T00:00:00.000Z',\n",
    "#                  '2021-07-01T00:00:00.000Z',\n",
    "#                  '2021-08-01T00:00:00.000Z',\n",
    "#                  '2021-09-01T00:00:00.000Z',\n",
    "#                  '2021-10-01T00:00:00.000Z',\n",
    "#                  '2021-11-01T00:00:00.000Z',\n",
    "#                  '2021-12-01T00:00:00.000Z',\n",
    "#                 ]\n",
    "\n",
    "# end_list =      ['2021-01-31T00:00:00.000Z',\n",
    "#                  '2021-02-28T00:00:00.000Z',\n",
    "#                  '2021-03-31T00:00:00.000Z',\n",
    "#                  '2021-04-30T00:00:00.000Z',\n",
    "#                  '2021-05-31T00:00:00.000Z',\n",
    "#                  '2021-06-30T00:00:00.000Z',\n",
    "#                  '2021-07-31T00:00:00.000Z',\n",
    "#                  '2021-08-31T00:00:00.000Z',\n",
    "#                  '2021-09-30T00:00:00.000Z',\n",
    "#                  '2021-10-31T00:00:00.000Z',\n",
    "#                  '2021-11-30T00:00:00.000Z',\n",
    "#                  '2021-12-31T00:00:00.000Z']\n",
    "\n",
    "\n",
    "# start_list = list(pd.date_range(start=\"2021-01-01T00:00:00.000Z\", end=\"2021-12-31T00:00:00.000Z\", freq='1H'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "emotional-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2021-05-01T00:00:00.000Z\n",
      "# of Tweets added from this response:  7\n",
      "Total # of Tweets added:  7\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2021-05-01T01:00:00.000Z\n",
      "# of Tweets added from this response:  11\n",
      "Total # of Tweets added:  18\n",
      "-------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8d2e103e2b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total # of Tweets added: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m#Since this is the final request, turn flag to false to move to the next time period.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Inputs for tweets\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "\n",
    "# keyword = \"(ethereum OR ether OR eth) lang:en -is:retweet -is:reply\"\n",
    "keyoword = \"(audiusproject OR audius OR $audio OR audiocoin) lang:en -is:retweet -is:reply\"\n",
    "datafile = \"../data/audio_twitter_data_hourly_full.csv\"\n",
    "\n",
    "max_results = 300 # total results per API call\n",
    "\n",
    "\n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(datafile, \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "# (Only the first time)\n",
    "# Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "# csvWriter.writerow(['author_id', 'created_at', 'tweet_id', 'text',\n",
    "#                     'like_count', 'quote_count', 'reply_count', 'retweet_count'])\n",
    "\n",
    "csvFile.close()\n",
    "\n",
    "for i in range(0,len(start_list)):\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 10_000 # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        \n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, datafile)\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, datafile)\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-setup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
